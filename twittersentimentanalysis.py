# -*- coding: utf-8 -*-
"""TwitterSentimentAnalysis.ipynb

Automatically generated by Colab.

"""

import pandas as pd

import numpy as np
import matplotlib.pyplot as plt

df=pd.read_csv('sentiment.csv')

df.head()

df.shape

df['label'].value_counts()

df[df['label']==0].head(10)

df[df['label']==1].head(10)

df.isnull().sum()

df.drop(['id'],axis=1,inplace=True)
df.head()

import string
string.punctuation

def clean(text):
    remv_pun=[char for char in text.lower() if char not in string.punctuation]
    remv_punc_join = ''.join(remv_pun)
    return remv_punc_join

clean(' @ Great beginning,,, takes! time,,,.   #run')

tweets_df_clean = df['tweet'].apply(clean)

tweets_df_clean[6]

tweets_df_clean.head()

!pip install nltk

import nltk
nltk.download('stopwords')
stopwords=nltk.corpus.stopwords.words('english')
print(stopwords[:5])

tweets_df = pd.DataFrame(tweets_df_clean)
tweets_df.columns

# stopwords treatment and converting the data into lower case
def stop(text):
    remv_stop = [a for a in text.split() if a.lower() not in stopwords]
    remv_stop_join = ' '.join(remv_stop)
    return remv_stop_join

stop(tweets_df['tweet'][0])

tweets_df_stopwords = tweets_df['tweet'].apply(stop)

tweets_df_stopwords[:2]

tweets_df_stopwords = pd.DataFrame(tweets_df_stopwords)
tweets_df_stopwords

from nltk.stem import PorterStemmer
st = PorterStemmer()

def steming(text):
    ste = [st.stem(word) for word in text.split()]
    ste_join = ' '.join(ste)
    return ste_join

tweets_df_stem = tweets_df_stopwords['tweet'].apply(steming)

tweets_df_stem[:2]

tweets_df_stopwords['tweet'][0]

from nltk.stem import WordNetLemmatizer

wl = WordNetLemmatizer()

import nltk
nltk.download('wordnet')

def lematize(text):
    ste = [wl.lemmatize(word) for word in text.split()]
    ste_join = ' '.join(ste)
    return ste_join

lematize('Dog keepss on barkings')

tweets_df_stopwords.iloc[:2]

tweets_df_stem = pd.DataFrame(tweets_df_stem)
tweets_df_stem.head()

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(max_features=5000)

sen = tweets_df_stem['tweet'].tolist()
len(sen)

from pandas import DataFrame

def document_matrix(text, vectorizer):
    mat = vectorizer.fit_transform(text)
    return DataFrame(mat.toarray())

m = document_matrix(sen,cv)
m.head()

from sklearn.feature_extraction.text import TfidfVectorizer


tfidf_vec = TfidfVectorizer(max_features=2500)

y= df['label']
y.head()

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(m,y,test_size=0.33,random_state=25)

x_train.shape
y_train.shape

from sklearn.naive_bayes import MultinomialNB
NaiveBclassifier = MultinomialNB()
NaiveBclassifier.fit(x_train,y_train)

y_pred_train = NaiveBclassifier.predict(x_train)

from sklearn.metrics import accuracy_score
#Accuracy Score

acc = accuracy_score(y_train, y_pred_train)
acc
